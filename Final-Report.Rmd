---
title: 'Case Study: Modeling Liquid Mechanics'
author: Conner Byrd, Eric Han, Ki Hyun, Sara Shao, Alex Shen, Mona Su, Dani Trejo,
  Steven Yuan
date: "10/12/2021"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning= FALSE,
                      message = FALSE,
                     fig.align = "center",
                     fig.width=5, 
                     fig.height=3)
```

```{r load-packages}
library(tidyverse)
library(patchwork)
library(mgcv)
library(MASS)
```


## Introduction

Our key research objectives include understanding and predicting how turbulence affects the dynamics of water droplets and ice crystals (how they collide and mix) in clouds. With our machine learning model, we are trying to infer the volume distribution of clusters within clouds. Our graphs have been included in the Appendix section of this report.


```{r, echo = FALSE}
train <- read.csv('data-train.csv')
train_data<- train %>%
mutate(Re = as.ordered(Re)) %>%
mutate(Fr = as.ordered(Fr))
```

## Methodology

### Initial Approach

We decided at the beginning to treat `Fr` and `Re` as ordered factors. It seemed to make sense to treat Fr as a categorical variable because we are given only three unique values. In terms of prediction and inference, we believe our models can still be generalized to `Fr` and `Re` values similar to the ones we are working with.

```{r, echo = TRUE}
first_lm_R1 <- lm(R_moment_1 ~ St + Re + Fr, data = train_data)
first_lm_R2 <- lm(R_moment_2 ~ St + Re + Fr, data = train_data)
first_lm_R3 <- lm(R_moment_3 ~ St + Re + Fr, data = train_data)
first_lm_R4 <- lm(R_moment_4 ~ St + Re + Fr, data = train_data)
summary(first_lm_R1)$r.squared
summary(first_lm_R4)$r.squared
```

We decided to initially fit the most basic linear model  with all three predictors to see what it would look like. The model for the first moment had a fairly high $R^2$ value (0.929), but the model for the fourth moment had a much lower $R^2$ value (0.425). Additionally, in our diagnostic plots (see Appendix: Section 2), we saw a pattern in the residuals vs. fitted values plots where the models would consistently under predict in some areas and over predict in others, indicating non-linearity. In addition, looking at the Normal Q-Q plots, the normality assumption also seemed to be violated for higher moments. This is consistent with the fact that our histogram of `St` in our EDA was not normally distributed. 

This information lead us to try using a GAM to model the relationship between the predictors and the 4 moments due to the increased flexbility GAMs provide. However, we knew that using GAMs made interpretability an issue, because interpreting a complex smooth function of a continuous predictor is very difficult 

### Variable transformation
As a result, we decided to use variable transformations and include interaction effects to make linear models with suitable model diagnostics for all 4 moments for the purpose of inference. To decide how to apply variable transformations, we observed the predictor and response variables, took a deeper look at the purpose of the study, and found three main problems.

1. Just converting `Fr`, and `Re` into categorical values may result in losing information as the exact numerical value could have high relationship with the moments. Furthermore, this model should be used later to predict behaviors of particles in environments with higher Re values.
2. The scale of `Fr`, `St`, and `Re` differs from each other. `Fr` ranges $[0, \infty)$, while `St` is $[0, 3]$. Furthermore, in the real world, `Re` could be in the scale of $10^7$.
3. The response variables (moments) are heavily skewed. For example, for the 4th moment, some response variables are at the scale of $10^{-10}$, while some are at the scale of $10^{10}$

As a result, we considered transformations on the potential predictor variables. For Froude number (`Fr`), a monotonic transformation of $1 - e^{-Fr}$ was made to change the range from $[0, \infty)$ to $[0, 1]$.
For Stokes number (`St`), as shown in the EDA (see Appendix: Section 1), log transformation was undertaken to address the skewness. For Reynolds number (`Re`), not only is the actual value in practice much larger than the given training data, but also within the training data itself, Reynolds number was much greater in scale. To address the difference in scale, log transformation was also made on Reynolds number.
```{r, echo = TRUE}
train_data <- train %>% 
    mutate(
        Re = log(Re),
        Fr = 1 - exp(-Fr),
        St = log(St)
    )
```

Transformation on the response variable also needed to take place before fitting the models. For the four moments, we have considered box-cox transformation to normalize the variables and adjust for the difference in scale. As the EDA shows (see Appendix: Section 1), the response variables have a skewed distribution. Thus, first, normalization was needed for the four moments. Moreover, the scale of the response variable becomes extremely larger as the order of the moments increase. (4th moment would have approximately to the power of 4 scale of the 1st moment) Hence, second, adjustments for scale were needed for the four moments. Box-cox transformation performs normalization and adjusts the scale of variables: apt for our response variable transformation.

However, the $\lambda$ for box-cox transformation needed to be specified for each of the four moments. We have calculated and specified the four different lambdas as the maximum log-likelihood estimate as below.

```{r, echo = TRUE}
lambda.1 = -0.0202
lambda.2 = -0.1010
lambda.3 = -0.0606
lambda.4 = -0.0606
```

### Forward AIC for Linear Models
For linear model, we tried doing forward selection with AIC to see if we could select a simpler model in case were overfitting, but our resulting models were the same as our input models (see Appendix: Section 2). We would also compare our final linear models with 4 GAM models (one for each moment) using 10-fold CV in order to find the best models for prediction. 

### Cross Validation
For each moment, we considered various combinations of predictor variables and interactions of the transformed variables. In order to compare the models and choose the most optimal ones, we used 10-fold cross validation to estimate the test error. A major reason for choosing cross validation was that cross validation can provide a direct estimate for the test estimate, even when we do not have a clear picture of the noises of the variables. Furthermore, it is less likely to result in overfitting compared to LOOCV, and computationally more feasible.

To perform a valid 10-fold cross validation, we first shuffled the training dataset to ensure that similar data do not tend to be included in the same fold. Then, we divided the training dataset into 10 folds and used the same folds to perform cross validation and obtain estimated test error for each model. Below shows how the folds were created.

```{r, echo = TRUE}
set.seed(3736)
shuffled_train <- train_data[sample(nrow(train_data)),]
folds <- cut(seq(1,nrow(train_data)),breaks=10,labels=FALSE)
```
 
At each fold, the validation set (8~9 data points for each fold) was reserved, and the other points were used to train a model. Then, the model was used to test on the validation set. RMSE was then calculated at each fold and the mean RSME was then calculated to compare the estimated test error with other model candidates. For each moment, the model with the lowest estimated test error was selected.

For each moment, the following linear models were considered to vary complexity and explore various possibilities: 

  1. GAM model without interaction
  
  2. GAM models with one interaction term on each combination of predictor variables
  
  3. GAM models with two interaction terms
  
  4. GAM model with all possible interaction terms

## Results

### Final Linear Models

The final four linear models, one for each moment takes the form of the 
equations below.

- First Moment:
$$
{E(R)^{\lambda_1} - 1 \over \lambda_1} \sim (1-e^{-Fr}) + log(St) + log(Re) + 
(1-e^{-Fr}) \times log(St) + log(St) \times log(Re) + (1-e^{-Fr}) \times log(Re)
$$

- Second Moment: 
$$
{E(R^2)^{\lambda_2} - 1 \over \lambda_2} \sim (1-e^{-Fr}) + log(St) + log(Re) + 
(1-e^{-Fr}) \times log(St) + log(St) \times log(Re) + (1-e^{-Fr}) \times log(Re)
$$

- Third Moment:
$$
{E(R^3)^{\lambda_3} - 1 \over \lambda_3} \sim (1-e^{-Fr}) + log(St) + log(Re) + 
(1-e^{-Fr}) \times log(St) + log(St) \times log(Re) + (1-e^{-Fr}) \times log(Re)
$$

- Fourth Moment:
$$
{E(R^4)^{\lambda_4} - 1 \over \lambda_4} \sim (1-e^{-Fr}) + log(St) + log(Re) + 
(1-e^{-Fr}) \times log(St) + log(St) \times log(Re) + (1-e^{-Fr}) \times log(Re)
$$

The model diagnostics plots of the four linear models shown in Appendix Figure 
2.7-14. The independence condition for regression is thought to be met since 
there does not seem to be a reason to believe otherwise about the population.
Moreover, from the diagnostics plots, the conditions for linearity and normality
of the residuals seem to have met. There still may be some heteroscedasticity 
present in the residuals; however, compared to the initial linear models, the 
variable transformation seems to have made the residuals substantially more 
homoscedastic.

The full output of each final linear model is shown in Appendix Figure 2.6

The two predictors ($log(Re)$ and $1 - e^{-Fr}$) and their interaction term 
seems to be statistically significant at $\alpha = 0.001$ level for all 4 final 
linear models. For the first moment, the interaction term between $log(St)$ and 
$1 - e^{-Fr}$ also seems to be significant at $\alpha = 0.05$ level.

```{r First_Moment, echo=FALSE}
train.t <- train_data
# setting lambda
lambda <- lambda.1
# CV set-up
shuffled_train <- train_data[sample(nrow(train_data)),] # training data
folds <- cut(seq(1,nrow(train_data)),breaks=10,labels=FALSE) # folds
rmse.cv.gam <- rep(0, 10) # error bins

# gam function with no interaction terms
gam.1.0 <- gam((R_moment_1 ^ lambda - 1)/lambda ~ s(St) + Re + Fr, 
               data = train.t)
# model diagnostics
#gam.check(gam.1.0)
# 10-fold CV test error est
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_1
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    gam_cv <- gam((R_moment_1 ^ lambda - 1)/lambda ~ s(St) + Re + Fr, 
                  data = trainData)
    pred_gam <- (lambda * predict(gam_cv, testData, type='response') + 1)^(1/lambda)
    rmse.cv.gam[i] = mean((pred_gam - y.test)^2)
}

cv.0 <- mean(rmse.cv.gam)

# gam function with St:Re
gam.1.1 <- gam((R_moment_1 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Re), data = train.t)
# model diagnostics
#gam.check(gam.1.1)
# 10-fold CV test error est
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_1
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    gam_cv <- gam((R_moment_1 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Re), data = trainData)
    pred_gam <- (lambda * predict(gam_cv, testData, type='response') + 1)^(1/lambda)
    rmse.cv.gam[i] = mean((pred_gam - y.test)^2)
}

cv.1 <- mean(rmse.cv.gam)

# gam function with St:Fr
gam.1.2 <- gam((R_moment_1 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Fr), data = train.t)
# model diagnostics
#gam.check(gam.1.2)
# 10-fold CV test error est
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_1
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    gam_cv <- gam((R_moment_1 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Fr), data = trainData)
    pred_gam <- (lambda * predict(gam_cv, testData, type='response') + 1)^(1/lambda)
    rmse.cv.gam[i] = mean((pred_gam - y.test)^2)
}

cv.2 <- mean(rmse.cv.gam)

# gam function with Re:Fr
gam.1.3 <- gam((R_moment_1 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + Re:Fr, 
               data = train.t)
# model diagnostics
#gam.check(gam.1.3)
# 10-fold CV test error est
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_1
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    gam_cv <- gam((R_moment_1 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + Re:Fr, 
                  data = trainData)
    pred_gam <- (lambda * predict(gam_cv, testData, type='response') + 1)^(1/lambda)
    rmse.cv.gam[i] = mean((pred_gam - y.test)^2)
}

cv.3 <- mean(rmse.cv.gam)

# gam function with St:Re, Re:Fr
gam.1.4 <- gam((R_moment_1 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Re) + Re:Fr, data = train.t)
# model diagnostics
#gam.check(gam.1.4)
# 10-fold CV test error est
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_1
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    gam_cv <- gam((R_moment_1 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Re) + Re:Fr, data = trainData)
    pred_gam <- (lambda * predict(gam_cv, testData, type='response') + 1)^(1/lambda)
    rmse.cv.gam[i] = mean((pred_gam - y.test)^2)
}

cv.4 <- mean(rmse.cv.gam)

# gam function with St:Fr, Re:Fr
gam.1.5 <- gam((R_moment_1 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Fr) + Re:Fr, data = train.t)
# model diagnostics
#gam.check(gam.1.5)
# 10-fold CV test error est
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_1
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    gam_cv <- gam((R_moment_1 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Fr) + Re:Fr, data = trainData)
    pred_gam <- (lambda * predict(gam_cv, testData, type='response') + 1)^(1/lambda)
    rmse.cv.gam[i] = mean((pred_gam - y.test)^2)
}

cv.5 <- mean(rmse.cv.gam)

# gam function with St:Fr, St:Re
gam.1.6 <- gam((R_moment_1 ^ lambda - 1)/lambda ~ s(St) + Re+ Fr + 
                   s(St, by = Fr) + s(St, by = Re), data = train.t)
# model diagnostics
#gam.check(gam.1.6)
# 10-fold CV test error est
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_1
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    gam_cv <- gam((R_moment_1 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Re) + s(St, by = Fr), data = trainData)
    pred_gam <- (lambda * predict(gam_cv, testData, type='response') + 1)^(1/lambda)
    rmse.cv.gam[i] = mean((pred_gam - y.test)^2)
}

cv.6 <- mean(rmse.cv.gam)

# gam function with all the interaction terms
gam.1.7 <- gam((R_moment_1 ^ lambda - 1)/lambda ~ s(St) + Re+ Fr + 
                   s(St, by = Fr) + s(St, by = Re) + Re:Fr, data = train.t)
# model diagnostics
#gam.check(gam.1.7)
# 10-fold CV test error est
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_1
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    gam_cv <- gam((R_moment_1 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Re) + s(St, by = Re) + Re:Fr, data = trainData)
    pred_gam <- (lambda * predict(gam_cv, testData, type='response') + 1)^(1/lambda)
    rmse.cv.gam[i] = mean((pred_gam - y.test)^2)
}

cv.7 <- mean(rmse.cv.gam)

# save the errors
CV_errors.gam  <- tibble(Model.0 = cv.0, Model.1 = cv.1, Model.2 = cv.2, 
                     Model.3 = cv.3, Model.4 = cv.4, Model.5 = cv.5, 
                     Model.6 = cv.6, Model.7 = cv.7)
```

```{r Second_Moment, echo=FALSE}
# setting lambda
lambda <- lambda.2

# gam function with no interaction terms
gam.2.0 <- gam((R_moment_2 ^ lambda - 1)/lambda ~ s(St) + Re + Fr, 
               data = train.t)
# model diagnostics
#gam.check(gam.2.0)
# 10-fold CV test error est
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_2
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    gam_cv <- gam((R_moment_2 ^ lambda - 1)/lambda ~ s(St) + Re + Fr, 
                  data = trainData)
    pred_gam <- (lambda * predict(gam_cv, testData, type='response') + 1)^(1/lambda)
    rmse.cv.gam[i] = mean((pred_gam - y.test)^2)
}

cv.0 <- mean(rmse.cv.gam)

# gam function with St:Re
gam.2.1 <- gam((R_moment_2 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Re), data = train.t)
# model diagnostics
#gam.check(gam.2.1)
# 10-fold CV test error est
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_2
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    gam_cv <- gam((R_moment_2 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Re), data = trainData)
    pred_gam <- (lambda * predict(gam_cv, testData, type='response') + 1)^(1/lambda)
    rmse.cv.gam[i] = mean((pred_gam - y.test)^2)
}

cv.1 <- mean(rmse.cv.gam)

# gam function with St:Fr
gam.2.2 <- gam((R_moment_2 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Fr), data = train.t)
# model diagnostics
#gam.check(gam.2.2)
# 10-fold CV test error est
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_2
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    gam_cv <- gam((R_moment_2 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Fr), data = trainData)
    pred_gam <- (lambda * predict(gam_cv, testData, type='response') + 1)^(1/lambda)
    rmse.cv.gam[i] = mean((pred_gam - y.test)^2)
}

cv.2 <- mean(rmse.cv.gam)

# gam function with Re:Fr
gam.2.3 <- gam((R_moment_2 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + Re:Fr, 
               data = train.t)
# model diagnostics
#gam.check(gam.2.3)
# 10-fold CV test error est
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_2
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    gam_cv <- gam((R_moment_2 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + Re:Fr, 
                  data = trainData)
    pred_gam <- (lambda * predict(gam_cv, testData, type='response') + 1)^(1/lambda)
    rmse.cv.gam[i] = mean((pred_gam - y.test)^2)
}

cv.3 <- mean(rmse.cv.gam)

# gam function with St:Re, Re:Fr
gam.2.4 <- gam((R_moment_2 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Re) + Re:Fr, data = train.t)
# model diagnostics
#gam.check(gam.2.4)
# 10-fold CV test error est
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_2
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    gam_cv <- gam((R_moment_2 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Re) + Re:Fr, data = trainData)
    pred_gam <- (lambda * predict(gam_cv, testData, type='response') + 1)^(1/lambda)
    rmse.cv.gam[i] = mean((pred_gam - y.test)^2)
}

cv.4 <- mean(rmse.cv.gam)

# gam function with St:Fr, Re:Fr
gam.2.5 <- gam((R_moment_2 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Fr) + Re:Fr, data = train.t)
# model diagnostics
#gam.check(gam.2.5)
# 10-fold CV test error est
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_2
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    gam_cv <- gam((R_moment_2 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Fr) + Re:Fr, data = trainData)
    pred_gam <- (lambda * predict(gam_cv, testData, type='response') + 1)^(1/lambda)
    rmse.cv.gam[i] = mean((pred_gam - y.test)^2)
}

cv.5 <- mean(rmse.cv.gam)

# gam function with St:Fr, St:Re
gam.2.6 <- gam((R_moment_2 ^ lambda - 1)/lambda ~ s(St) + Re+ Fr + 
                   s(St, by = Fr) + s(St, by = Re), data = train.t)
# model diagnostics
#gam.check(gam.2.6)
# 10-fold CV test error est
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_2
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    gam_cv <- gam((R_moment_2 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Re) + s(St, by = Fr), data = trainData)
    pred_gam <- (lambda * predict(gam_cv, testData, type='response') + 1)^(1/lambda)
    rmse.cv.gam[i] = mean((pred_gam - y.test)^2)
}

cv.6 <- mean(rmse.cv.gam)

# gam function with all the interaction terms
gam.2.7 <- gam((R_moment_2 ^ lambda - 1)/lambda ~ s(St) + Re+ Fr + 
                   s(St, by = Fr) + s(St, by = Re) + Re:Fr, data = train.t)
# model diagnostics
#gam.check(gam.2.7)
# 10-fold CV test error est
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_2
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    gam_cv <- gam((R_moment_2 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Re) + s(St, by = Re) + Re:Fr, data = trainData)
    pred_gam <- (lambda * predict(gam_cv, testData, type='response') + 1)^(1/lambda)
    rmse.cv.gam[i] = mean((pred_gam - y.test)^2)
}

cv.7 <- mean(rmse.cv.gam)

# save the errors
CV_errors.gam <- CV_errors.gam %>% 
    add_row(Model.0 = cv.0, Model.1 = cv.1, Model.2 = cv.2, Model.3 = cv.3, 
            Model.4 = cv.4, Model.5 = cv.5, Model.6 = cv.6, Model.7 = cv.7)
```

```{r Third_Moment, echo=FALSE}
# setting lambda
lambda <- lambda.3

# gam function with no interaction terms
gam.3.0 <- gam((R_moment_3 ^ lambda - 1)/lambda ~ s(St) + Re + Fr, 
               data = train.t)
# model diagnostics
#gam.check(gam.3.0)
# 10-fold CV test error est
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_3
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    gam_cv <- gam((R_moment_3 ^ lambda - 1)/lambda ~ s(St) + Re + Fr, 
                  data = trainData)
    pred_gam <- (lambda * predict(gam_cv, testData, type='response') + 1)^(1/lambda)
    rmse.cv.gam[i] = mean((pred_gam - y.test)^2)
}

cv.0 <- mean(rmse.cv.gam)

# gam function with St:Re
gam.3.1 <- gam((R_moment_3 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Re), data = train.t)
# model diagnostics
#gam.check(gam.3.1)
# 10-fold CV test error est
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_3
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    gam_cv <- gam((R_moment_3 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Re), data = trainData)
    pred_gam <- (lambda * predict(gam_cv, testData, type='response') + 1)^(1/lambda)
    rmse.cv.gam[i] = mean((pred_gam - y.test)^2)
}

cv.1 <- mean(rmse.cv.gam)

# gam function with St:Fr
gam.3.2 <- gam((R_moment_3 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Fr), data = train.t)
# model diagnostics
#gam.check(gam.3.2)
# 10-fold CV test error est
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_3
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    gam_cv <- gam((R_moment_3 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Fr), data = trainData)
    pred_gam <- (lambda * predict(gam_cv, testData, type='response') + 1)^(1/lambda)
    rmse.cv.gam[i] = mean((pred_gam - y.test)^2)
}

cv.2 <- mean(rmse.cv.gam)

# gam function with Re:Fr
gam.3.3 <- gam((R_moment_3 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + Re:Fr, 
               data = train.t)
# model diagnostics
#gam.check(gam.3.3)
# 10-fold CV test error est
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_3
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    gam_cv <- gam((R_moment_3 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + Re:Fr, 
                  data = trainData)
    pred_gam <- (lambda * predict(gam_cv, testData, type='response') + 1)^(1/lambda)
    rmse.cv.gam[i] = mean((pred_gam - y.test)^2)
}

cv.3 <- mean(rmse.cv.gam)

# gam function with St:Re, Re:Fr
gam.3.4 <- gam((R_moment_3 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Re) + Re:Fr, data = train.t)
# model diagnostics
#gam.check(gam.3.4)
# 10-fold CV test error est
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_3
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    gam_cv <- gam((R_moment_3 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Re) + Re:Fr, data = trainData)
    pred_gam <- (lambda * predict(gam_cv, testData, type='response') + 1)^(1/lambda)
    rmse.cv.gam[i] = mean((pred_gam - y.test)^2)
}

cv.4 <- mean(rmse.cv.gam)

# gam function with St:Fr, Re:Fr
gam.3.5 <- gam((R_moment_3 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Fr) + Re:Fr, data = train.t)
# model diagnostics
#gam.check(gam.3.5)
# 10-fold CV test error est
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_3
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    gam_cv <- gam((R_moment_3 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Fr) + Re:Fr, data = trainData)
    pred_gam <- (lambda * predict(gam_cv, testData, type='response') + 1)^(1/lambda)
    rmse.cv.gam[i] = mean((pred_gam - y.test)^2)
}

cv.5 <- mean(rmse.cv.gam)

# gam function with St:Fr, St:Re
gam.3.6 <- gam((R_moment_3 ^ lambda - 1)/lambda ~ s(St) + Re+ Fr + 
                   s(St, by = Fr) + s(St, by = Re), data = train.t)
# model diagnostics
#gam.check(gam.3.6)
# 10-fold CV test error est
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_3
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    gam_cv <- gam((R_moment_3 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Re) + s(St, by = Fr), data = trainData)
    pred_gam <- (lambda * predict(gam_cv, testData, type='response') + 1)^(1/lambda)
    rmse.cv.gam[i] = mean((pred_gam - y.test)^2)
}

cv.6 <- mean(rmse.cv.gam)

# gam function with all the interaction terms
gam.3.7 <- gam((R_moment_3 ^ lambda - 1)/lambda ~ s(St) + Re+ Fr + 
                   s(St, by = Fr) + s(St, by = Re) + Re:Fr, data = train.t)
# model diagnostics
#gam.check(gam.3.7)
# 10-fold CV test error est
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_3
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    gam_cv <- gam((R_moment_3 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Re) + s(St, by = Re) + Re:Fr, data = trainData)
    pred_gam <- (lambda * predict(gam_cv, testData, type='response') + 1)^(1/lambda)
    rmse.cv.gam[i] = mean((pred_gam - y.test)^2)
}

cv.7 <- mean(rmse.cv.gam)

# save the errors
CV_errors.gam <- CV_errors.gam %>% 
    add_row(Model.0 = cv.0, Model.1 = cv.1, Model.2 = cv.2, Model.3 = cv.3, 
            Model.4 = cv.4, Model.5 = cv.5, Model.6 = cv.6, Model.7 = cv.7)
```

```{r Fourth_Moment, echo=FALSE}
# setting lambda
lambda <- lambda.4

# gam function with no interaction terms
gam.4.0 <- gam((R_moment_4 ^ lambda - 1)/lambda ~ s(St) + Re + Fr, 
               data = train.t)
# model diagnostics
#gam.check(gam.4.0)
# 10-fold CV test error est
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_4
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    gam_cv <- gam((R_moment_4 ^ lambda - 1)/lambda ~ s(St) + Re + Fr, 
                  data = trainData)
    pred_gam <- (lambda * predict(gam_cv, testData, type='response') + 1)^(1/lambda)
    rmse.cv.gam[i] = mean((pred_gam - y.test)^2)
}

cv.0 <- mean(rmse.cv.gam)

# gam function with St:Re
gam.4.1 <- gam((R_moment_4 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Re), data = train.t)
# model diagnostics
#gam.check(gam.4.1)
# 10-fold CV test error est
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_4
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    gam_cv <- gam((R_moment_4 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Re), data = trainData)
    pred_gam <- (lambda * predict(gam_cv, testData, type='response') + 1)^(1/lambda)
    rmse.cv.gam[i] = mean((pred_gam - y.test)^2)
}

cv.1 <- mean(rmse.cv.gam)

# gam function with St:Fr
gam.4.2 <- gam((R_moment_4 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Fr), data = train.t)
# model diagnostics
#gam.check(gam.4.2)
# 10-fold CV test error est
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_4
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    gam_cv <- gam((R_moment_4 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Fr), data = trainData)
    pred_gam <- (lambda * predict(gam_cv, testData, type='response') + 1)^(1/lambda)
    rmse.cv.gam[i] = mean((pred_gam - y.test)^2)
}

cv.2 <- mean(rmse.cv.gam)

# gam function with Re:Fr
gam.4.3 <- gam((R_moment_4 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + Re:Fr, 
               data = train.t)
# model diagnostics
#gam.check(gam.4.3)
# 10-fold CV test error est
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_4
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    gam_cv <- gam((R_moment_4 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + Re:Fr, 
                  data = trainData)
    pred_gam <- (lambda * predict(gam_cv, testData, type='response') + 1)^(1/lambda)
    rmse.cv.gam[i] = mean((pred_gam - y.test)^2)
}

cv.3 <- mean(rmse.cv.gam)

# gam function with St:Re, Re:Fr
gam.4.4 <- gam((R_moment_4 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Re) + Re:Fr, data = train.t)
# model diagnostics
#gam.check(gam.4.4)
# 10-fold CV test error est
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_4
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    gam_cv <- gam((R_moment_4 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Re) + Re:Fr, data = trainData)
    pred_gam <- (lambda * predict(gam_cv, testData, type='response') + 1)^(1/lambda)
    rmse.cv.gam[i] = mean((pred_gam - y.test)^2)
}

cv.4 <- mean(rmse.cv.gam)

# gam function with St:Fr, Re:Fr
gam.4.5 <- gam((R_moment_4 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Fr) + Re:Fr, data = train.t)
# model diagnostics
#gam.check(gam.4.5)
# 10-fold CV test error est
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_4
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    gam_cv <- gam((R_moment_4 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Fr) + Re:Fr, data = trainData)
    pred_gam <- (lambda * predict(gam_cv, testData, type='response') + 1)^(1/lambda)
    rmse.cv.gam[i] = mean((pred_gam - y.test)^2)
}

cv.5 <- mean(rmse.cv.gam)

# gam function with St:Fr, St:Re
gam.4.6 <- gam((R_moment_4 ^ lambda - 1)/lambda ~ s(St) + Re+ Fr + 
                   s(St, by = Fr) + s(St, by = Re), data = train.t)
# model diagnostics
#gam.check(gam.4.6)
# 10-fold CV test error est
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_4
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    gam_cv <- gam((R_moment_4 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Re) + s(St, by = Fr), data = trainData)
    pred_gam <- (lambda * predict(gam_cv, testData, type='response') + 1)^(1/lambda)
    rmse.cv.gam[i] = mean((pred_gam - y.test)^2)
}

cv.6 <- mean(rmse.cv.gam)

# gam function with all the interaction terms
gam.4.7 <- gam((R_moment_4 ^ lambda - 1)/lambda ~ s(St) + Re+ Fr + 
                   s(St, by = Fr) + s(St, by = Re) + Re:Fr, data = train.t)
# model diagnostics
#gam.check(gam.4.7)
# 10-fold CV test error est
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_4
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    gam_cv <- gam((R_moment_4 ^ lambda - 1)/lambda ~ s(St) + Re + Fr + 
                   s(St, by = Re) + s(St, by = Re) + Re:Fr, data = trainData)
    pred_gam <- (lambda * predict(gam_cv, testData, type='response') + 1)^(1/lambda)
    rmse.cv.gam[i] = mean((pred_gam - y.test)^2)
}

cv.7 <- mean(rmse.cv.gam)

# save the errors
CV_errors.gam <- CV_errors.gam %>% 
    add_row(Model.0 = cv.0, Model.1 = cv.1, Model.2 = cv.2, Model.3 = cv.3, 
            Model.4 = cv.4, Model.5 = cv.5, Model.6 = cv.6, Model.7 = cv.7)
```

#### Final GAM Models

As shown from the results of the 10-fold CV test error estimation in Appendix
Figure 3.1, for each moment of R, the 6th model was selected as the final GAM
model. The 6th model was configured with all three predictors and two 
interaction terms: one between $log(Re)$ and $1 - e^{-Fr}$, and the other 
between $s(log(St))$ and $1 - e^{-Fr}$. As indicated by the `s()`, smoothing 
function of GAM was applied to $log(St)$ to aid fitting the data better.

The model diagnostic plots of the four GAM models using function `gam.check()`
showed that the conditions for the GAM model has met.

Though, due to the presence of a smoothing function, interpretation of the 
coefficients is challenging, GAM could still be an addition to the research
if it brings more prediction power.

#### Test MSE Estimates: LM vs. GAM

Comparing the estimated testing error between the 4 final LM models and the 
4 final GAM models using 10-fold CV yielded that for the first moment the GAM
model is preferred. For the second, third, and fourth moments, the linear models
were preferred. 

## Appendix

### Figure 1.1: Distribution of St
```{r, echo = FALSE}
ggplot(data = train, mapping = aes(x = St)) +
  geom_histogram(fill="#9fc5e8" )+
   theme_minimal()
```

We will try using a log transform on the St variable since the distribution for the St variable is not normally distributed.

### Figure 1.2: R Moments and Re Colored by Fr
```{r}
scatter1 <- ggplot(data = train, mapping = aes(x = Re, y = R_moment_1, color = factor(Fr))) +
  geom_point(show.legend = FALSE)+
   theme_minimal()
scatter2 <- ggplot(data = train, mapping = aes(x = Re, y = R_moment_2, color = factor(Fr))) +
  geom_point()+
   theme_minimal()
scatter3 <- ggplot(data = train, mapping = aes(x = Re, y = R_moment_3, color = factor(Fr))) +
  geom_point(show.legend = FALSE)+
   theme_minimal()
scatter4 <- ggplot(data = train, mapping = aes(x = Re, y = R_moment_4, color = factor(Fr))) +
  geom_point()+
   theme_minimal()

(scatter1 + scatter2) / (scatter3 + scatter4)
```
### Figure 1.3: R Moments and St Colored by Fr
```{r}
scatter5 <- ggplot(data = train, mapping = aes(x = St, y = R_moment_1, color = factor(Fr))) +
  geom_point(show.legend = FALSE) +
  geom_smooth(method = lm, se = F, show.legend = FALSE)+
   theme_minimal()
scatter6 <- ggplot(data = train, mapping = aes(x = St, y = R_moment_2, color = factor(Fr))) +
  geom_point() +
  geom_smooth(method = lm, se = F)+
   theme_minimal()
scatter7 <- ggplot(data = train, mapping = aes(x = St, y = R_moment_3, color = factor(Fr))) +
  geom_point(show.legend = FALSE) +
  geom_smooth(method = lm, se = F, show.legend = FALSE)+
   theme_minimal()
scatter8 <- ggplot(data = train, mapping = aes(x = St, y = R_moment_4, color = factor(Fr))) +
  geom_point() +
  geom_smooth(method = lm, se = F)+
   theme_minimal()

(scatter5 + scatter6) / (scatter7 + scatter8)
```
The graphs above show some evidence of interactions, so we will explore interaction terms in our model.

### Figure 2.1-2: Diagnostic Plots For Inital Linear Model of R Moment 1

```{r}
plot(first_lm_R1, 1:2)
```

### Figure 2.3-4: Diagnostic Plots For Inital Linear Model of R Moment 4

```{r}
plot(first_lm_R4, 1:2)
```

Because the linearity condition is not fulfilled in the above Residuals vs. Fitted plots, we will consider performing a log transformation on our response variables (R moments 1-4).

### Figure 2.5: Forward Selection With AIC On Linear Models

```{r, echo = TRUE}
lm1 <- lm((R_moment_1^lambda.1 - 1) / 
            lambda.1 ~ St + Re + Fr + St*Fr + Fr*Re + St*Re, data = train_data)
step(lm1, direction = "forward")

lm2 <- lm((R_moment_2^lambda.2 - 1) / 
            lambda.2 ~ St + Re + Fr + St*Fr + Fr*Re + St*Re, data = train_data)
#step(lm2, direction = "forward")

lm3 <- lm((R_moment_3^lambda.2 - 1) / 
            lambda.3  ~ St + Re + Fr + St*Fr + Fr*Re + St*Re, data = train_data)
#step(lm3, direction = "forward")

lm4 <- lm((R_moment_4^lambda.4 - 1) / 
            lambda.4 ~ St + Re + Fr + St*Fr + Fr*Re + St*Re, data = train_data)
step(lm4, direction = "forward")
```

### Firgure 2.6: Final Linear Models Output

```{r, echo = TRUE}
lm1 <- lm((R_moment_1^lambda.1 - 1) / 
            lambda.1 ~ St + Re + Fr + Fr*Re + St*Fr + Re*St, data = train_data)
summary(lm1)
lm2 <- lm((R_moment_2^lambda.2 - 1) / 
            lambda.2 ~ St + Re + Fr + Fr*Re + St*Fr + Re*St, data = train_data)
summary(lm2)
lm3 <- lm((R_moment_3^lambda.3 - 1) / 
            lambda.3 ~ St + Re + Fr + Fr*Re + St*Fr + Re*St, data = train_data)
summary(lm3)
lm4 <- lm((R_moment_4^lambda.4 - 1) / 
            lambda.4 ~ St + Re + Fr + Fr*Re + St*Fr + Re*St, data = train_data)
summary(lm4)
```

### Figure 2.7-8: Diagnostic Plots For Final Linear Model of R Moment 1

```{r}
plot(lm1, 1:2)
```

### Figure 2.9-10: Diagnostic Plots For Final Linear Model of R Moment 2

```{r}
plot(lm2, 1:2)
```

### Figure 2.11-12: Diagnostic Plots For Final Linear Model of R Moment 3

```{r}
plot(lm3, 1:2)
```

### Figure 2.13-14: Diagnostic Plots For Final Linear Model of R Moment 4

```{r}
plot(lm4, 1:2)
```

### Figure 3.1: 10-fold CV estimated Test Error for GAM Models of Each Moment

```{r}
CV_errors.gam
```