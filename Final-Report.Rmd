---
title: 'Case Study: Modeling Liquid Mechanics'
author: Conner Byrd, Eric Han, Ki Hyun, Sara Shao, Alex Shen, Mona Su, Dani Trejo,
  Steven Yuan
date: "10/12/2021"
output:
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                     warning = FALSE,
                     message = FALSE,
                     fig.align = "center",
                     fig.width=5, 
                     fig.height=3)
```

```{r load-packages}
library(tidyverse)
library(patchwork)
```


## Introduction

Our key research objectives include understanding and predicting how turbulence affects the dynamics of water droplets and ice crystals (how they collide and mix) in clouds. With our machine learning model, we are trying to infer the volume distribution of clusters within clouds.

To do this, we began by doing some basic Exploratory Data Analysis on the three predictor variables: Reynolds number (`Re`), gravitational acceleration (`Fr`), and particle characteristic (`St`). (Our graphs are included in Appendix: Section 1)

```{r}
train <- read.csv('data-train.csv')
```

## Methodology

### Initial Approach

We decided at the beginning to treat Fr and Re as ordered factors. It seemed to make sense to treat Fr as a categorical variable because we are given only three unique values. In terms of prediction and inference, we believe our models can still be generalized to Fr and Re values similar to the ones we are working with.

```{r, echo = TRUE}
first_lm_R1 <- lm(R_moment_1 ~ St + Re + Fr, data = train_data)
first_lm_R2 <- lm(R_moment_2 ~ St + Re + Fr, data = train_data)
first_lm_R3 <- lm(R_moment_3 ~ St + Re + Fr, data = train_data)
first_lm_R4 <- lm(R_moment_4 ~ St + Re + Fr, data = train_data)
summary(first_lm_R1)$r.squared
summary(first_lm_R4)$r.squared
```

We decided to initially fit the most basic linear model  with all three predictors to see what it would look like. The model for the first moment had a fairly high $R^2$ value (0.929), but the model for the fourth moment had a much lower $R^2$ value (0.425). Additionally, in our diagnostic plots (see Appendix: Section 2), we saw a pattern in the residuals vs fitted values plots where the models would consistently under predict in some areas and over predict in others, indicating non-linearity. In addition, looking at the Normal Q-Q plots, the normality assumption also seemed to be violated for higher moments. This is consistent with the fact that our histogram of St in our EDA was not normally distributed. 

This information lead us to try using a GAM to model the relationship between the predictors and the 4 moments due to the increased flexbility GAMs provide. However, we knew that using GAMs made interpretability an issue, because interpreting a complex smooth function of a continuous predictor is very hard. 

### Variable transformation
As a result, we decided to use variable transformations and include interaction effects to make linear models with suitable model diagnostics for all 4 moments for the purpose of inference. To decide how to apply variable transformations, we observed the predictor and response variables, took a deeper look at the purpose of the study, and found three main problems.

1. Just converting Fr, and Re into categorical values may result in losing information as the exact numerical value could have high relationship with the moments. Furthermore, this model should be used later to predict behaviors of particles in environments with higher Re values.
2. The scale of Fr, St, and Re differs from each other. Fr ranges $[0, \infty)$, while St is $[0, 3]$. Furthermore, in real world, Re could be in the scale of $10^7$.
3. The response variables (moments) are heavily skewed. For example, for the 4th moment, some response variables are at the scale of $10^{-10}$, while some are at the scale of $10^{10}$

As a result, we considered transformations on the potential predictor variables. For Froude number (Fr), a monotonic transformation of $1 - e^{-Fr}$ was made to change the range from $[0, \infty)$ to $[0, 1]$.
For Stokes number (St), as shown in the EDA, log transformation was undertaken to address the skewness. For Reynolds number (Re), not only is the actual value in practice much larger than the given training data, but also within the training data itself, Reynolds number was much greater in scale. To address the difference in scale, log transformation was also made on Reynolds number.
```{r, echo = TRUE}
train_data <- train %>% 
    mutate(
        Re = log(Re),
        Fr = 1 - exp(-Fr),
        St = log(St)
    )
```

Transformation on the response variable also needed to take place before fitting the models. For the four moments, we have considered box-cox transformation to normalize the variables and adjust for the difference in scale. As the EDA shows, the response variables have a skewed distribution. Thus, first, normalization was needed for the four moments. Moreover, the scale of the response variable becomes extremely larger as the order of the moments increase. (4th moment would have approximately to the power of 4 scale of the 1st moment) Hence, second, adjustments for scale were needed for the four moments. Box-cox transformation performs normalization and adjusts the scale of variables: apt for our response variable transformation.

However, the $\lambda$ for box-cox transformation needed to be specified for each of the four moments. We have calculated and speficied the four different lambdas as the maximum log-likelihood estimate as below.

```{r, echo = TRUE}
lambda.1 = -0.0202
lambda.2 = -0.1010
lambda.3 = -0.0606
lambda.4 = -0.0606
```

### Forward AIC for Linear Models
For linear model, we tried doing forward selection with AIC to see if we could select a simpler model in case were overfitting, but our resulting models were the same as our input models (see Appendix: Section 2). We would also compare our final linear models with 4 GAM models (one for each moment) using 10-fold CV in order to find the best models for prediction. 

### Cross Validation
For each moments, we considered various combinations of predictor variables and interactions of the transformed variables. In order to compare the models and choose the most optimal ones, we used 10-fold cross validation to estimate the test error. A major reason for choosing cross validation was that cross validation can provide a direct estimate for the test estimate, even when we do not have a clear picture of the noises of the variables. Furthermore, it is less likely to result in overfitting compared to LOOCV, and computationally more feasible.

To perform a valid 10-fold cross validation, we first shuffled the training dataset to ensure that similar data do not tend to be included in the same fold. Then, we divided the training dataset into 10 folds and used the same folds to perform cross validation and obtain estimated test error for each model. Below is how the folds were created.

```{r, echo = TRUE}
set.seed(3736)
shuffled_train <- train_data[sample(nrow(train_data)),]
folds <- cut(seq(1,nrow(train_data)),breaks=10,labels=FALSE)
```
 
At each fold, the validation set (8~9 data points for each fold) was reserved, and the other points were used to train a model. Then, the model was used to test on the validation set. RMSE was then calculated at each fold and the mean RSME was then calculated to compare the estimated test error with other model candidates. For each moment, the model with the lowest estimated test error was selected.

For each moment, the following linear models were considered to vary complexity and explore various possibilities: 
1. GAM model without interaction
2. GAM models with one interaction term on each combination of predictor variables
3. GAM models with two interaction terms
4. GAM model with all possible interaction terms

## Results

### Final Linear Model

```{r, echo = TRUE}
lm1 <- lm((R_moment_1^lambda.1 - 1) / lambda.1 ~ St + Re + Fr + Fr*Re + St*Fr + Re*St, data = train_data)
summary(lm1)
lm2 <- lm((R_moment_2^lambda.2 - 1) / lambda.2 ~ St + Re + Fr + Fr*Re + St*Fr + Re*St, data = train_data)
summary(lm2)$r.squared
lm3 <- lm((R_moment_3^lambda.3 - 1) / lambda.3 ~ St + Re + Fr + Fr*Re + St*Fr + Re*St, data = train_data)
summary(lm3)$r.squared
lm4 <- lm((R_moment_4^lambda.4 - 1) / lambda.4 ~ St + Re + Fr + Fr*Re + St*Fr + Re*St, data = train_data)
summary(lm4)$r.squared
```

All three of our main predictors: Fr, St, and Re, seem to be significant predictors of the R moments. A one percent increase in Stokes number is associated with 0.146% increase in R moment 1, holding all other predictors constant. When the Reynolds number is 224, the R moment 1 is expected to decrease by 403% from when the Reynolds number is 90, holding all other predictors constant. When the Reynolds number is 398, the R moment 1 is expected to increase by 64% compared to when the Reynolds number is 90. 

The interaction terms Fr\*St and Re\*Fr seem to be significant as well. This is consistent with what we expected from our EDA. When Fr is 0.3, the relationship between R moment 1 and St is positive, however, when Fr is infinity, the relationship between R moment 1 and St is negative. Additionally, we see that when the Froud number is 0.3 and the Reynolds number is 224, the R moment 1 is expected to be an additional 24% lower compared to when either of those conditions are not met.

What these results mean for the research question is that Reynolds number (`Re`), gravitational acceleration (`Fr`), and particle characteristic (`St`) all predict cluster formation in clouds to some degree, even after accounting for the effects of each of the other variables. We can also conclude that direction of the relationship between cluster density and Stokes number is different based on what the Froud number is.

#### Predicted Test Error For R Moments 1-4 With Linear Model

$\newline$
Predicted Mean-Squared Error For R Moment 1
```{r warning = FALSE}
# error
rmse.cv.lm <- rep(0, 10)

# Cross validation
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_1
    trainData <- shuffled_train[-testIndexes, ]
    
    #Use the test and train data
    lm_cv <- lm((R_moment_1^lambda.1 - 1) / lambda.1 ~ St + Re + Fr + Fr*Re + St*Fr + Re*St, data = trainData)
    pred_lm <- (predict(lm_cv, testData, type='response') * lambda.1 + 1)^(1/lambda.1)
    rmse.cv.lm[i] = mean((pred_lm - y.test)^2)
}

mean(rmse.cv.lm)
```

Predicted Mean-Squared Error For R Moment 2
```{r warning = FALSE}
# error
rmse.cv.lm <- rep(0, 10)

# Cross validation
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_2
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    lm_cv <- lm(log(R_moment_2) ~ St + Re + Fr + St*Fr + Re*Fr + St*Re, data = trainData)
    pred_lm <- exp(predict(lm_cv, testData, type='response'))
    rmse.cv.lm[i] = mean((pred_lm - y.test)^2)
    
    #Use the test and train data
    lm_cv <- lm((R_moment_2^lambda.2 - 1) / lambda.2 ~ St + Re + Fr + Fr*Re + St*Fr + Re*St, data = trainData)
    pred_lm <- (predict(lm_cv, testData, type='response') * lambda.2 + 1)^(1/lambda.2)
    rmse.cv.lm[i] = mean((pred_lm - y.test)^2)
}

mean(rmse.cv.lm)
```

Predicted Mean-Squared Error For R Moment 3
```{r warning = FALSE}
# error
rmse.cv.lm <- rep(0, 10)

# Cross validation
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_3
    trainData <- shuffled_train[-testIndexes, ]
   
    #Use the test and train data
    lm_cv <- lm(log(R_moment_3) ~ St + Re + Fr + Fr*Re + St*Fr + Re*St, data = trainData)
    pred_lm <- exp(predict(lm_cv, testData, type='response'))
    rmse.cv.lm[i] = mean((pred_lm - y.test)^2)
}

mean(rmse.cv.lm)
```

Predicted Mean-Squared Error For R Moment 4
```{r warning = FALSE}
# error
rmse.cv.lm <- rep(0, 10)

# Cross validation
for(i in 1:10){
    #Segment your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- shuffled_train[testIndexes, ]
    y.test <- testData$R_moment_4
    trainData <- shuffled_train[-testIndexes, ]
   
    lm_cv <- lm((R_moment_4^lambda.4 - 1) / lambda.4 ~ St + Re + Fr + Fr*Re + St*Fr + Re*St, data = trainData)
    pred_lm <- (predict(lm_cv, testData, type='response') * lambda.4 + 1)^(1/lambda.4)
    rmse.cv.lm[i] = mean((pred_lm - y.test)^2)
}

mean(rmse.cv.lm)
```


## Appendix

### Figure 1.1: Distribution of St
```{r, echo = FALSE}
ggplot(data = train, mapping = aes(x = St)) +
  geom_histogram()
```

We will try using a log transform on the St variable since the distribution for the St variable is not normally distributed.

### Figure 1.2: R Moments and Re Colored by Fr
```{r}
scatter1 <- ggplot(data = train, mapping = aes(x = Re, y = R_moment_1, color = factor(Fr))) +
  geom_point(show.legend = FALSE)
scatter2 <- ggplot(data = train, mapping = aes(x = Re, y = R_moment_2, color = factor(Fr))) +
  geom_point()
scatter3 <- ggplot(data = train, mapping = aes(x = Re, y = R_moment_3, color = factor(Fr))) +
  geom_point(show.legend = FALSE)
scatter4 <- ggplot(data = train, mapping = aes(x = Re, y = R_moment_4, color = factor(Fr))) +
  geom_point()

(scatter1 + scatter2) / (scatter3 + scatter4)
```
### Figure 1.3: R Moments and St Colored by Fr
```{r}
scatter5 <- ggplot(data = train, mapping = aes(x = St, y = R_moment_1, color = factor(Fr))) +
  geom_point(show.legend = FALSE) +
  geom_smooth(method = lm, se = F, show.legend = FALSE)
scatter6 <- ggplot(data = train, mapping = aes(x = St, y = R_moment_2, color = factor(Fr))) +
  geom_point() +
  geom_smooth(method = lm, se = F)
scatter7 <- ggplot(data = train, mapping = aes(x = St, y = R_moment_3, color = factor(Fr))) +
  geom_point(show.legend = FALSE) +
  geom_smooth(method = lm, se = F, show.legend = FALSE)
scatter8 <- ggplot(data = train, mapping = aes(x = St, y = R_moment_4, color = factor(Fr))) +
  geom_point() +
  geom_smooth(method = lm, se = F)

(scatter5 + scatter6) / (scatter7 + scatter8)
```
The graphs above show some evidence of interactions, so we will explore interaction terms in our model.

### Figure 2.1-2: Diagnostic Plots For Inital Linear Model of R Moment 1

```{r}
plot(first_lm_R1, 1:2)
```

### Figure 2.3-4: Diagnostic Plots For Inital Linear Model of R Moment 4

```{r}
plot(first_lm_R4, 1:2)
```

Because the linearity condition is not fulfilled in the above Residuals vs. Fitted plots, we will consider performing a log transformation on our response variables (R moments 1-4).

### Figure 2.5: Forward Selection With AIC On Linear Models

```{r, echo = TRUE}
lm1 <- lm((R_moment_1^lambda.1 - 1) / lambda.1 ~ St + Re + Fr + St*Fr + Fr*Re + St*Re, data = train_data)
step(lm1, direction = "forward")

lm2 <- lm((R_moment_2^lambda.2 - 1) / lambda.2 ~ St + Re + Fr + St*Fr + Fr*Re + St*Re, data = train_data)
#step(lm2, direction = "forward")

lm3 <- lm((R_moment_3^lambda.2 - 1) / lambda.3  ~ St + Re + Fr + St*Fr + Fr*Re + St*Re, data = train_data)
#step(lm3, direction = "forward")

lm4 <- lm((R_moment_4^lambda.4 - 1) / lambda.4 ~ St + Re + Fr + St*Fr + Fr*Re + St*Re, data = train_data)
step(lm4, direction = "forward")
```

