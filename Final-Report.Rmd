---
title: "Final Report"
author: "Conner Byrd, Eric Han, Ky Hyun, Sara Shao, Alex Shen, Mona Su, Dani Trejo, Steven Yuan"
date: "10/12/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                     warning = FALSE,
                     message = FALSE,
                     fig.align = "center")
```

```{r load-packages}
library(tidyverse)
library(patchwork)
```


## Introduction

Our key research objectives include understanding and predicting how turbulence affects the dynamics of water droplets and ice crystals (how they collide and mix) in clouds. With our machine learning model, we are trying to infer the volume distribution of clusters within clouds.

To do this, we began by doing some basic Exploratory Data Analysis on the three predictor variables: Reynolds number (`Re`), gravitational acceleration (`Fr`), and particle characteristic (`St`). (Our graphs are included in Appendix: Section 1)

```{r}
train <- read.csv('data-train.csv')
```

```{r}
head(train)
```


## Methodology

BAD LINEAR MODEL SHOWS TRANSFORMATIONS NEEDED AND GAM MIGHT BE A GOOD IDEA

```{r}
train_data <- train %>% 
  mutate(Fr = as.ordered(Fr)) %>% 
  mutate(Re = as.ordered(Re))
```

```{r}
simplest_lm <- lm(log(R_moment_1) ~ St + Re + Fr, data = train_data)
plot(simplest_lm)
```

We decided to initially fit the most basic linear model to see what it would look like. We see that there may be an issue with heteroskedasticity in the residuals vs fitted values plot. In addition, looking at the Normal Q-Q plot, the normality assumption also seems to be violated. This information, combined with the seemingly nonlinear relationship between our only continuous predictor St and the 4 moments as seen in the EDA, leads us to try to use a GAM to model the relationship between the predictors and the 4 moments due to the increased flexbility GAMs provide.


We knew that using GAMs made interpretability an issue, because interpreting a complex smooth function of a continuous predictor is very hard. As a result, we decided to use variable transformations and interaction effects to make linear models with suitable model diagnostics for all 4 moments for the purpose of inference, but we would also compare these models with 4 GAM models (one for each moment) in order to find the best models for prediction. 



## Results

## Appendix

### Figure 1.1
```{r, echo = FALSE}
ggplot(data = train, mapping = aes(x = St)) +
  geom_histogram()
```

We will try using a log transform on the St variable since the distribution for the St variable is not normally distributed.

### Figure 1.2
```{r}
scatter1 <- ggplot(data = train, mapping = aes(x = Re, y = R_moment_1, color = factor(Fr))) +
  geom_point(show.legend = FALSE)
scatter2 <- ggplot(data = train, mapping = aes(x = Re, y = R_moment_2, color = factor(Fr))) +
  geom_point()
scatter3 <- ggplot(data = train, mapping = aes(x = Re, y = R_moment_3, color = factor(Fr))) +
  geom_point(show.legend = FALSE)
scatter4 <- ggplot(data = train, mapping = aes(x = Re, y = R_moment_4, color = factor(Fr))) +
  geom_point()

(scatter1 + scatter2) / (scatter3 + scatter4)
```
### Figure 1.3
```{r}
scatter5 <- ggplot(data = train, mapping = aes(x = St, y = R_moment_1, color = factor(Fr))) +
  geom_point(show.legend = FALSE) +
  geom_smooth(method = lm, se = F, show.legend = FALSE)
scatter6 <- ggplot(data = train, mapping = aes(x = St, y = R_moment_2, color = factor(Fr))) +
  geom_point() +
  geom_smooth(method = lm, se = F)
scatter7 <- ggplot(data = train, mapping = aes(x = St, y = R_moment_3, color = factor(Fr))) +
  geom_point(show.legend = FALSE) +
  geom_smooth(method = lm, se = F, show.legend = FALSE)
scatter8 <- ggplot(data = train, mapping = aes(x = St, y = R_moment_4, color = factor(Fr))) +
  geom_point() +
  geom_smooth(method = lm, se = F)

(scatter5 + scatter6) / (scatter7 + scatter8)
```
The graphs above show some evidence of interactions, so we will explore interaction terms in our model.
